# Coderev vs coderev vs ripgrep vs RAG — Reproducible Benchmark (Local)

Date: February 3, 2026

## Goal
Evaluate whether Coderev improves *LLM‑ready retrieval* versus coderev, ripgrep, and a simple RAG baseline on this repo.

## Reproducible Setup

1) **Index Coderev** (local DB in `.coderev/`):
```bash
cargo build --release
target/release/coderev index --path . --database .coderev/bench_coderev.db --json
```

2) **Ensure coderev index exists** (already configured in `.coderev/config.yaml`).

3) **LMStudio** (required for coderev and RAG):
- Start LMStudio server at `http://127.0.0.1:1234`.
- Export models:
```bash
export LMSTUDIO_ENDPOINT=http://127.0.0.1:1234/v1
export LMSTUDIO_EMBED_MODEL=text-embedding-nomic-embed-text-v1.5
export LMSTUDIO_CHAT_MODEL=<your-chat-model>
```

4) **Run all benchmarks**:
```bash
bench/run_all.sh
```

This produces artifacts under `bench/results/`.

## Queries
See `bench/queries.json` (5 intent‑style queries).

## Latency (time -p)
From `bench/results/*time` (seconds):

**Coderev**
- watch-background: 1.93
- mcp-server: 1.49
- embedding-resolver: 1.42
- agent-setup: 1.55
- default-db: 1.54

**ripgrep**
- watch-background: 0.02
- mcp-server: 0.00
- embedding-resolver: 0.00
- agent-setup: 0.00
- default-db: 0.00

**coderev**
- watch-background: 0.41
- mcp-server: 0.17
- embedding-resolver: 0.18
- agent-setup: 0.23
- default-db: 0.26

**RAG (chunked embeddings)**
- watch-background: 0.14
- mcp-server: 0.13
- embedding-resolver: 0.14
- agent-setup: 0.13
- default-db: 0.15

## Notes
- Coderev results are symbol‑level; rg results are raw line matches.
- This benchmark uses a deterministic query list and fixed scripts under `bench/`.

## QA Evaluation (LLM with retrieved context)

The QA run uses the LMStudio chat model `liquid/lfm2.5-1.2b` and the contexts generated by each tool.
Artifacts are saved as `bench/results/qa_<tool>_<query>.json`.

Key findings (qualitative):
- **Coderev** answers tended to cite the exact symbol or definition when present (e.g., default DB path).
- **coderev/RAG** sometimes returned relevant but less precise explanations depending on chunk selection.
- **rg** answers were often terse because context windows were derived from line matches only.

Important caveat: QA quality is **model‑dependent** and should be re‑run with your target LLM.

---

## Scorecard (Quick Look)

See `bench/REPORT.md` for the generated summary table (latency + QA citation presence).
